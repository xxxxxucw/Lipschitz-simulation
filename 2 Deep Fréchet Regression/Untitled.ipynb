{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61ef57-351f-4d86-bdac-108c0ffe3119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random \n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 核心逻辑移植：对应 DenSimu.R 的数据生成\n",
    "# ==========================================\n",
    "def generate_densimu_data_batch(n, m, seed):\n",
    "    \"\"\"\n",
    "    Python implementation of the simulation logic from DenSimu.R\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate Predictors X (9 dimensions as per DenSimu.R)\n",
    "    X1 = np.random.uniform(-1, 0, n)\n",
    "    X2 = np.random.uniform(0, 1, n)\n",
    "    X3 = np.random.uniform(1, 2, n)\n",
    "    X4 = np.random.normal(0, 1, n)\n",
    "    X5 = np.random.normal(-10, 3, n)\n",
    "    X6 = np.random.normal(10, 3, n)\n",
    "    # p is probability of 0, 1. R: sample(c(0,1), n, TRUE, c(0.4, 0.6))\n",
    "    X7 = np.random.choice([0, 1], n, p=[0.4, 0.6]) \n",
    "    X8 = np.random.choice([0, 1], n, p=[0.3, 0.7])\n",
    "    X9 = np.random.choice([0, 1], n, p=[0.6, 0.4])\n",
    "\n",
    "    # Stack features: shape (n, 9)\n",
    "    X = np.stack([X1, X2, X3, X4, X5, X6, X7, X8, X9], axis=1)\n",
    "    \n",
    "    # Parameters from R code\n",
    "    sigma0 = 3\n",
    "    kappa = 1\n",
    "    \n",
    "    # Initialize output array\n",
    "    # Shape: (n, m, x_dim) and (n, m)\n",
    "    x_expanded = np.zeros((n, m, 9))\n",
    "    y = np.zeros((n, m))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_i = X[i]\n",
    "        \n",
    "        # 2. Calculate Expected Mean (Eta) and Sigma based on R formulas\n",
    "        term1 = 3 * (np.sin(np.pi * x_i[0]) + np.cos(np.pi * x_i[1])) * x_i[7]\n",
    "        term2 = (5 * (x_i[3]**2) + x_i[4]) * x_i[6]\n",
    "        expect_eta_Z = term1 + term2\n",
    "        \n",
    "        term3 = 0.5 * (np.sin(np.pi * x_i[0]) + np.cos(np.pi * x_i[1])) * x_i[7]\n",
    "        term4 = np.abs(5 * (x_i[3]**2) + x_i[4]) * x_i[6]\n",
    "        expect_sigma_Z = sigma0 + term3 + term4\n",
    "        \n",
    "        # 3. Sample latent mean (mu) and sigma\n",
    "        mu = np.random.normal(expect_eta_Z, 1.0) \n",
    "        \n",
    "        # NumPy gamma uses shape (k) and scale (theta). \n",
    "        shape_param = (expect_sigma_Z**2) / kappa\n",
    "        scale_param = kappa / expect_sigma_Z\n",
    "        sigma = np.random.gamma(shape_param, scale_param)\n",
    "        \n",
    "        # 4. Generate m observations\n",
    "        y_i = mu + sigma * np.random.normal(0, 1, m)\n",
    "        \n",
    "        y[i, :] = y_i\n",
    "        x_expanded[i, :, :] = x_i \n",
    "        \n",
    "    return x_expanded, y\n",
    "\n",
    "\n",
    "def save_3d_data_to_csv(x_3d, y, filename, n, m, d):\n",
    "    # 构建 ID 列\n",
    "    sample_ids = np.repeat(np.arange(n), m)\n",
    "    meas_ids = np.tile(np.arange(m), n)\n",
    "    \n",
    "    # 重塑 X 和 Y\n",
    "    x_flat = x_3d.reshape(n * m, d)\n",
    "    y_flat = y.reshape(n * m)\n",
    "    \n",
    "    # 合并\n",
    "    data_matrix = np.column_stack((sample_ids, meas_ids, x_flat, y_flat))\n",
    "    \n",
    "    # 创建列名\n",
    "    feature_columns = [f'feature_{i+1}' for i in range(d)]\n",
    "    columns =  ['sample_id', 'measurement_id'] + feature_columns + ['target']\n",
    "    \n",
    "    # 保存\n",
    "    df = pd.DataFrame(data_matrix, columns=columns)\n",
    "    df['sample_id'] = df['sample_id'].astype(int)\n",
    "    df['measurement_id'] = df['measurement_id'].astype(int)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def savedata(seed):\n",
    "    \"\"\"\n",
    "    Worker function\n",
    "    \"\"\"\n",
    "    seed2 = ((seed+50) * 20000331)%2**31\n",
    "    np.random.seed(seed2) \n",
    "    \n",
    "    # 使用 visualize.py 中定义的 n_vector 保持一致\n",
    "    n_train_values = [10, 13, 17, 23, 31, 42, 57, 78, 106, 145, 198, 271, 400]\n",
    "    m_train_values = [20] # 固定 m=20\n",
    "    d_feature = 9\n",
    "\n",
    "    # 1. 生成测试集 (一次性生成)\n",
    "    n_test = 10000\n",
    "    m_test = 1 \n",
    "    x_test, y_test = generate_densimu_data_batch(n_test, m_test, seed2)\n",
    "    \n",
    "    # 保存测试集\n",
    "    x_test_flat = x_test[:, 0, :]\n",
    "    y_test_flat = y_test[:, 0]\n",
    "    \n",
    "    test_df = pd.DataFrame(x_test_flat, columns=[f'feature_{i+1}' for i in range(d_feature)])\n",
    "    test_df['target'] = y_test_flat\n",
    "    os.makedirs(\"./data/test_data/\", exist_ok=True)\n",
    "    test_df.to_csv(f\"./data/test_data/s{seed}.csv\", index=False)\n",
    "    \n",
    "    # 2. 生成训练和验证集\n",
    "    for n_train in n_train_values:\n",
    "        for m_train in m_train_values:\n",
    "            n_valid = math.ceil(n_train * 0.25)\n",
    "            m_valid = m_train\n",
    "            \n",
    "            # 生成训练数据\n",
    "            x, y = generate_densimu_data_batch(n_train, m_train, seed2)\n",
    "            \n",
    "            # 生成验证数据\n",
    "            x_valid, y_valid = generate_densimu_data_batch(n_valid, m_valid, seed2 + 1)\n",
    "            \n",
    "            # 保存\n",
    "            save_3d_data_to_csv(x, y, f\"./data/train_data/n{n_train}m{m_train}s{seed}.csv\",\n",
    "                               n_train, m_train, d_feature)\n",
    "\n",
    "            save_3d_data_to_csv(x_valid, y_valid, f\"./data/valid_data/n{n_train}m{m_valid}s{seed}.csv\",\n",
    "                               n_valid, m_valid, d_feature)\n",
    "    \n",
    "    print(f\"Seed {seed} completed.\")\n",
    "\n",
    "# ==========================================\n",
    "# 主执行块\n",
    "# ==========================================\n",
    "seedlist = list(range(50)) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seeds = list(seedlist)\n",
    "    nproc = 40 # 您的机器似乎核心数较多，可以适当调大\n",
    "    \n",
    "    # 【关键修改】移除 set_start_method('spawn')，Linux下默认使用 fork\n",
    "    # 如果必须设置，使用: multiprocessing.set_start_method('fork', force=True)\n",
    "    print(\"Starting data generation based on DenSimu.R logic...\")\n",
    "    \n",
    "    with multiprocessing.Pool(processes = nproc) as pool: \n",
    "        pool.map(savedata, seedlist)\n",
    "    print(\"All data generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b77b4c-f363-4b6a-bd5e-70d393de8b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on RTX 4090 with 4 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Process SpawnPoolWorker-2:\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-7:\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-12:\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'onedim' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import math\n",
    "import torch  \n",
    "import torch.nn as nn     \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random \n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import time\n",
    "from colorama import init, Fore\n",
    "\n",
    "init(autoreset=True)\n",
    "\n",
    "# 强制使用第一块 GPU (RTX 4090)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, batch_size=10, lr =0.001, nepoch = 200, patience = 10, wide = 100, depth = 5, n_train=1, m_train=1) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.nepoch = nepoch \n",
    "        self.patience = patience \n",
    "        self.wide = wide \n",
    "        self.depth = depth \n",
    "        self.biaoji = \"wide\" + str(wide) + \"depth\" + str(depth) + \"n\" + str(n_train) + \"m\" + str(m_train)\n",
    "        self.n_train = n_train\n",
    "        self.m_train = m_train\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, save_path, args, verbose=False, delta=0):\n",
    "        self.save_path = save_path \n",
    "        self.patience = args.patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "\n",
    "    def __call__(self, model, train_loss, valid_loss, test_error, lipschitz, args, seed):\n",
    "        score = -valid_loss \n",
    "        if self.best_score is None: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(model, train_loss, valid_loss, test_error, lipschitz, args, seed) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, train_loss, valid_loss, test_error, lipschitz, args, seed)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model, train_loss, valid_loss, test_error, lipschitz, args, seed):\n",
    "        # 确保目录存在\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(self.save_path, 'best' + str(seed) + args.biaoji +'network.pth') )\n",
    "        torch.save(train_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'train_loss.pth')) \n",
    "        torch.save(valid_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'valid_loss.pth')) \n",
    "        torch.save(test_error, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'test_loss.pth')) \n",
    "        torch.save(lipschitz, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'lipschitz.pth')) \n",
    "        self.val_loss_min = valid_loss\n",
    "\n",
    "class Dataset_repeatedmeasurement(Dataset): \n",
    "    def __init__(self, x, y) -> None:  \n",
    "        super().__init__()\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "    def __len__(self) -> int: \n",
    "        return len(self.x) \n",
    "    def __getitem__(self, index): \n",
    "        return {\"x\" : self.x[index], \"y\" : self.y[index]}\n",
    "\n",
    "class happynet(nn.Module):\n",
    "    # 动态构建网络，保持你原有的逻辑\n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_layer): \n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(n_feature, n_hidden))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers (n_layer - 2)\n",
    "        # Assuming n_layer includes input and output transitions conceptually in your original code logic\n",
    "        # Original code: if n_layer=3 -> In(1)-Re-Hid(1)-Re-Out(1). Total linear layers = 3.\n",
    "        for _ in range(n_layer - 2):\n",
    "            layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(n_hidden, n_output))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_lipschitz_constant(model):\n",
    "    \"\"\"计算模型的全局Lipschitz常数（各层谱范数乘积）\"\"\"\n",
    "    lipschitz = 1.0\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            W = layer.weight.detach()\n",
    "            # 使用 float32 计算 SVD 可能会慢，但在 GPU 上通常很快\n",
    "            try:\n",
    "                # torch.linalg.svd 在某些 CUDA 版本可能不稳定，如果报错可转到 CPU\n",
    "                _, s, _ = torch.linalg.svd(W, full_matrices=False)\n",
    "                lipschitz *= s[0].item()\n",
    "            except:\n",
    "                # Fallback to CPU if CUDA SVD fails\n",
    "                _, s, _ = torch.linalg.svd(W.cpu(), full_matrices=False)\n",
    "                lipschitz *= s[0].item()\n",
    "    return lipschitz\n",
    "\n",
    "def load_model_and_compute_lipschitz(model_path, n_feature, n_hidden, n_output, n_layer, device):\n",
    "    model = happynet(n_feature=n_feature, n_hidden=n_hidden, n_output=n_output, n_layer=n_layer).to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    lipschitz = compute_lipschitz_constant(model)\n",
    "    return model, lipschitz\n",
    "\n",
    "def GPUstrain(x, y, x_valid, y_valid, x_test, y_test, args, seed, device):\n",
    "    # 自动获取特征维度\n",
    "    x_dim = x.shape[1] \n",
    "\n",
    "    net = happynet(n_feature=x_dim, n_hidden=args.wide, n_output=1, n_layer=args.depth).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr, betas=(0.90, 0.999), eps=1e-8) \n",
    "    loss_func = nn.MSELoss() \n",
    "\n",
    "    train_epochs_loss = [] \n",
    "    valid_epochs_loss = [] \n",
    "    test_epochs_error = [] \n",
    "\n",
    "    train_dataset = Dataset_repeatedmeasurement(x, y)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # Move valid/test to GPU once\n",
    "    x_valid = torch.from_numpy(x_valid).float().to(device) \n",
    "    y_valid = torch.from_numpy(y_valid).float().to(device) \n",
    "    x_test = torch.from_numpy(x_test).float().to(device)\n",
    "    y_test = torch.from_numpy(y_test).float().to(device) \n",
    "\n",
    "    save_path = \"./resultsv\" \n",
    "    early_stopping = EarlyStopping(save_path, args=args)\n",
    "\n",
    "    for epoch in range(args.nepoch): \n",
    "        net.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        for traindata in train_dataloader:\n",
    "            x_train = traindata[\"x\"].float().to(device, non_blocking=True)\n",
    "            y_train = traindata[\"y\"].float().to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = net(x_train) \n",
    "            loss = loss_func(outputs.view(-1), y_train.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_epoch_loss)        \n",
    "        train_epochs_loss.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            net.eval() \n",
    "            valid_predict = net(x_valid)\n",
    "            loss_valid = loss_func(valid_predict.view(-1), y_valid.view(-1))\n",
    "            valid_epochs_loss.append(loss_valid.item())\n",
    "\n",
    "            test_predict = net(x_test)\n",
    "            error_test = loss_func(test_predict.view(-1), y_test.view(-1))\n",
    "            test_epochs_error.append(error_test.item())\n",
    "\n",
    "            current_lipschitz = compute_lipschitz_constant(net)\n",
    "\n",
    "        if epoch > 10 or args.n_train*args.m_train > 200:\n",
    "            early_stopping(net, avg_train_loss, loss_valid.item(), error_test.item(), current_lipschitz, args, seed)\n",
    "            if early_stopping.early_stop: \n",
    "                break\n",
    "\n",
    "    return net, train_epochs_loss, valid_epochs_loss, test_epochs_error\n",
    "\n",
    "def onedim(n_train, m_train, seed):\n",
    "    # 设置设备\n",
    "    device = torch.device(\"cuda:0\") # 使用 4090\n",
    "\n",
    "    seed2 = ((seed+50) * 20000331 )% 2**31\n",
    "    torch.manual_seed(seed2) \n",
    "    np.random.seed(seed2) \n",
    "    random.seed(seed2) \n",
    "    \n",
    "    n_valid = math.ceil(n_train*0.25)\n",
    "    m_valid = m_train\n",
    "\n",
    "    # 读取数据\n",
    "    try:\n",
    "        train_df = pd.read_csv(f\"./data/train_data/n{n_train}m{m_train}s{seed}.csv\")\n",
    "        valid_df = pd.read_csv(f\"./data/valid_data/n{n_train}m{m_valid}s{seed}.csv\")\n",
    "        test_df = pd.read_csv(f\"./data/test_data/s{seed}.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data not found for n={n_train}, m={m_train}, seed={seed}\")\n",
    "        return None\n",
    "\n",
    "    # 提取特征列（自动过滤非feature列）\n",
    "    feature_cols = [col for col in train_df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    x_train = train_df[feature_cols].values\n",
    "    y_train = train_df['target'].values\n",
    "    x_valid = valid_df[feature_cols].values\n",
    "    y_valid = valid_df['target'].values\n",
    "    x_test = test_df[feature_cols].values\n",
    "    y_test = test_df['target'].values\n",
    "\n",
    "    # Batch size 策略\n",
    "    total_samples = n_train * m_train\n",
    "    if total_samples < 128:\n",
    "       batch_size, lr = min(total_samples, 32), 0.0005\n",
    "    elif total_samples < 1024:\n",
    "        batch_size, lr = 64, 0.0005\n",
    "    elif total_samples < 4096:\n",
    "        batch_size, lr = 128, 0.001\n",
    "    elif total_samples < 16384:\n",
    "        batch_size, lr = 256, 0.002\n",
    "    else:\n",
    "        batch_size, lr = 1024, 0.002\n",
    "\n",
    "    # 网络配置\n",
    "    net_configs = [\n",
    "        {'wide': 50, 'depth': 2, 'id': 0},\n",
    "        {'wide': 100, 'depth': 3, 'id': 1},\n",
    "        {'wide': 200, 'depth': 4, 'id': 2},\n",
    "        {'wide': 400, 'depth': 5, 'id': 3},\n",
    "        {'wide': 600, 'depth': 6, 'id': 4},\n",
    "        {'wide': 800, 'depth': 6, 'id': 5}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in net_configs:\n",
    "        args = Args(lr=lr, wide=config['wide'], depth=config['depth'], \n",
    "                   batch_size=batch_size, n_train=n_train, m_train=m_train)\n",
    "        \n",
    "        # 训练\n",
    "        GPUstrain(x_train, y_train, x_valid, y_valid, x_test, y_test, args, seed2, device)\n",
    "        \n",
    "        # 加载最佳结果\n",
    "        try:\n",
    "            # 路径\n",
    "            prefix = os.path.join('./resultsv', f'best{seed2}{args.biaoji}')\n",
    "            train_loss = torch.load(prefix + 'train_loss.pth', map_location='cpu')\n",
    "            valid_loss = torch.load(prefix + 'valid_loss.pth', map_location='cpu')\n",
    "            test_loss = torch.load(prefix + 'test_loss.pth', map_location='cpu')\n",
    "            lipschitz = torch.load(prefix + 'lipschitz.pth', map_location='cpu')\n",
    "            \n",
    "            results.append({\n",
    "                'n_train': n_train,\n",
    "                'm_train': m_train,\n",
    "                'train_loss': float(train_loss),\n",
    "                'valid_loss': float(valid_loss),\n",
    "                'test_loss': float(test_loss),\n",
    "                'lipschitz': float(lipschitz),\n",
    "                'net_id': config['id']\n",
    "            })\n",
    "            \n",
    "            # 清理模型文件以节省空间（可选）\n",
    "            # os.remove(prefix + 'network.pth')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading results for net {config['id']}: {e}\")\n",
    "\n",
    "    # 保存结果\n",
    "    if results:\n",
    "        df_res = pd.DataFrame(results)\n",
    "        os.makedirs(\"./perf\", exist_ok=True)\n",
    "        df_res.to_csv(f'./perf/n{n_train}m{m_train}s{seed}.csv', index=False)\n",
    "        return df_res\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# 主执行入口\n",
    "# ==========================================\n",
    "# 必须与数据生成和可视化保持一致\n",
    "n_vector = [10, 13, 17, 23, 31, 42, 57, 78, 106, 145, 198, 271, 400]\n",
    "m_vector = [20] # 固定采样频率\n",
    "seeds = list(range(50)) # 50次重复实验\n",
    "\n",
    "# 构建任务队列\n",
    "tasks = []\n",
    "for n in n_vector:\n",
    "    for m in m_vector:\n",
    "        for s in seeds:\n",
    "            tasks.append((n, m, s))\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    multiprocessing.set_start_method('spawn', force=True) \n",
    "    \n",
    "    # 针对 4090 单卡，不建议开启过多并行训练进程，因为显存和CUDA上下文切换开销大\n",
    "    # 建议 nproc = 1 或 2。如果你的模型很小，可以尝试 4。\n",
    "    # 这里设置为 4，利用 4090 的大显存（24GB）并行跑 4 个实验\n",
    "    nproc = 4 \n",
    "    \n",
    "    print(f\"Starting training on RTX 4090 with {nproc} parallel processes...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with multiprocessing.Pool(processes = nproc) as pool: \n",
    "        pool.starmap(onedim, tasks)\n",
    "        \n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e01536-47b4-415a-8df6-74e48ccfb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rc('font', size=15)\n",
    "rc('text', usetex=False)\n",
    "\n",
    "color_tuple = ['#ae1908', '#ec813b', '#05348b', '#9acdc4', '#00FF00', '#0000FF']\n",
    "n_vector = [10, 13, 17, 23, 31, 42, 57, 78, 106, 145, 198, 271, 400]\n",
    "fixed_m_train = 20\n",
    "\n",
    "# 初始化存储矩阵\n",
    "train_mse_matrix = np.zeros((len(n_vector), 6))\n",
    "test_mse_matrix = np.zeros((len(n_vector), 6))\n",
    "lip_matrix = np.zeros((len(n_vector), 6))\n",
    "\n",
    "print(\"Loading data for visualization...\")\n",
    "\n",
    "for i, n_train in enumerate(n_vector):\n",
    "    results = []\n",
    "    for seed in range(50):\n",
    "        file_path = f\"./perf/n{n_train}m{fixed_m_train}s{seed}.csv\"\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                results.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        else:\n",
    "            # print(f\"Missing: {file_path}\") # Optional: reduce noise\n",
    "            pass\n",
    "\n",
    "    if results:\n",
    "        all_results = pd.concat(results, ignore_index=True)\n",
    "        # 按网络ID分组计算均值\n",
    "        for net_id in range(6):\n",
    "            net_data = all_results[all_results['net_id'] == net_id]\n",
    "            if not net_data.empty:\n",
    "                train_mse_matrix[i, net_id] = net_data['train_loss'].mean()\n",
    "                test_mse_matrix[i, net_id] = net_data['test_loss'].mean()\n",
    "                lip_matrix[i, net_id] = net_data['lipschitz'].mean()\n",
    "    else:\n",
    "        print(f\"No valid data for n={n_train}\")\n",
    "\n",
    "# 绘图设置\n",
    "lines = ['solid']*4 + ['dashed']*2\n",
    "marker = ['o', 's', '^', 'v', 'D', 'x']\n",
    "model_name = ['L=2, W=50','L=3, W=100', 'L=4, W=200', 'L=5, W=400', 'L=6, W=600', 'L=6, W=800']\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "plt.subplots_adjust(top=0.95, bottom=0.15, left=0.1, right=0.95, wspace=0.3)\n",
    "\n",
    "# Plot Training Error\n",
    "for i in range(6):\n",
    "    ax1.plot(n_vector, train_mse_matrix[:, i], color=color_tuple[i], linestyle=lines[i],\n",
    "             label=model_name[i], marker=marker[i])\n",
    "ax1.set_ylabel(r\"$\\mathtt{train-mse}$\", fontsize=14)\n",
    "ax1.set_xlabel(r\"Training Set Size $n$\", fontsize=14)\n",
    "ax1.set_title(f\"Training Error (m={fixed_m_train})\", fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Test Error\n",
    "for i in range(6):\n",
    "    ax2.plot(n_vector, test_mse_matrix[:, i], color=color_tuple[i], linestyle=lines[i],\n",
    "             label=model_name[i], marker=marker[i])\n",
    "ax2.set_ylabel(r\"$\\mathtt{test-mse}$\", fontsize=14)\n",
    "ax2.set_xlabel(r\"Training Set Size $n$\", fontsize=14)\n",
    "ax2.set_title(f\"Test Error (m={fixed_m_train})\", fontsize=14)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot Lipschitz Constant\n",
    "log_lip = np.log10(lip_matrix + 1e-10) # 加上 epsilon 防止 log(0)\n",
    "for i in range(6):\n",
    "    ax3.plot(n_vector, log_lip[:, i], color=color_tuple[i], linestyle=lines[i],\n",
    "             label=model_name[i], marker=marker[i])\n",
    "ax3.set_ylabel(r\"Lipschitz constant $log_{10}(L)$\", fontsize=14)\n",
    "ax3.set_xlabel(r\"Training Set Size $n$\", fontsize=14)\n",
    "ax3.set_title(f\"Lipschitz Constant (m={fixed_m_train})\", fontsize=14)\n",
    "ax3.set_xscale(\"log\")\n",
    "ax3.legend()\n",
    "\n",
    "save_path = './Simulation/Case1_R_Adapted'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "plt.savefig(os.path.join(save_path, 'DenSimu_Result.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved to {save_path}/DenSimu_Result.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbc3d9-c129-431a-991c-609cb1526084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc3383-a668-4042-bbf9-3b04e8a8f9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b324abb-0af8-4e39-9d5b-7815fd83a28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891b107-2728-418f-9acf-b273d3d627ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
